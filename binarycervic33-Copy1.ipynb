{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import keras\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications import ResNet50\n",
    "\n",
    "\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from keras_radam import RAdam\n",
    "\n",
    "from pyimagesearch.learningratefinder import LearningRateFinder\n",
    "#from pyimagesearch import config\n",
    "from pyimagesearch.clr_callback import CyclicLR\n",
    "from keras import backend as K\n",
    "\n",
    "import sys\n",
    "from imutils import paths\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "# # construct the argument parser and parse the arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument(\"-d\", \"--dataset\", required=True,\n",
    "# \thelp=\"path to input dataset\")\n",
    "# #ap.add_argument(\"-a\", \"--augment\", type=int, default=-1,\n",
    "# #\thelp=\"whether or not 'on the fly' data augmentation should be used\")\n",
    "# #ap.add_argument(\"-p\", \"--plot\", type=str, default=\"plot.png\",\n",
    "# #\thelp=\"path to output loss/accuracy plot\")\n",
    "# args = vars(ap.parse_args())\n",
    "# # # Using Resnet50 and initialised with weights of imagenet\n",
    "# # ## images in smear 2005 are resized to 224 x224 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# grab the list of images in our dataset directory, then initialize\n",
    "# the list of data (i.e., images) and class images\n",
    "print(\"[INFO] loading images...\")\n",
    "#imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
    "imagePaths = list(paths.list_images('./dataset_binary/'))\n",
    "#print(imagePaths)\n",
    "data = []\n",
    "labels = []\n",
    " \n",
    "# loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "\t# extract the class label from the filename, load the image, and\n",
    "\t# resize it to be a fixed 64x64 pixels, ignoring aspect ratio\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\t#image = load_img(imagePath, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "\t#image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "\t#image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\timage = cv2.imread(imagePath,1)\n",
    "\timage = cv2.resize(image, (224, 224))\n",
    "\timage= preprocess_input(image)\n",
    "\t# update the data and labels lists, respectively\n",
    "\tdata.append(image)\n",
    "\t\n",
    "\tlabels.append(label)\n",
    "\n",
    "\n",
    "#print(imagePaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n",
      "(917, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data, dtype=\"float\") \n",
    "print('loaded data')\n",
    "print(data.shape)\n",
    "#print(len(data))\n",
    "#print(len(labels))\n",
    "# one-hot encode them\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "labels1=labels\n",
    "labels = np_utils.to_categorical(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# convert the data into a NumPy array, then preprocess it by scaling\n",
    "# all pixel intensities to the range [0, 1]\n",
    "#data = np.array(data, dtype=\"float\") / 255.0\n",
    " \n",
    "# encode the labels (which are currently strings) as integers and then\n",
    "\n",
    " \n",
    "# partition the data into training and testing splits using 75% of\n",
    "# the data for training and the remaining 25% for testing\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "        test_size=0.20, random_state=10, shuffle=True)\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 50.\n",
    "                }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thamizharasi/anaconda3/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "# Resnet initialisation with imagenet\n",
    "\n",
    "img_height,img_width = 224,224\n",
    "num_classes = 2\n",
    "input_shape= (img_height,img_width,3)\n",
    "#base_model=ResNet50(weights='imagenet',include_top=False,input_shape= (img_height,img_width,3)) #imports the mobilenet model and discards the \n",
    "\n",
    "restnet = ResNet50(include_top=False, weights='imagenet', input_shape= (img_height,img_width,3))\n",
    "\n",
    "\n",
    "output = restnet.layers[-1].output\n",
    "output = keras.layers.Flatten()(output)\n",
    "output= Dense(1048,activation='relu')(output)\n",
    "output= Dense(256,activation='relu')(output)\n",
    "\n",
    "preds=Dense(num_classes,activation='softmax')(output ) #final layer with softmax activatio\n",
    "\n",
    "model = Model(inputs=restnet.input, outputs=preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Freeze the layers except the last 4 layers\n",
    "for layer in restnet.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Created function to computer F1 SCORE\n",
    "\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# ## Compiled model using Adam optimizer and computed accuracy and f1 score\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "#opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / EPOCHS)\n",
    "\n",
    "# optimiser intitialisation\n",
    "\n",
    "\n",
    "\n",
    "#decay_rate = INIT_LR \n",
    "#decay_rate = INIT_LR / EPOCHS \n",
    "\n",
    "# learning rate schedule\n",
    "\n",
    "# initial_lrate = 0.1\n",
    "# drop = 0.5\n",
    "# epochs_drop = 10.0\n",
    "\n",
    "# def step_decay(epoch):\n",
    "# \t#lrrate = math.floor(initial_lrate/3.0)    \n",
    "# \tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "# \t#decay_rate.append(lrrate)\n",
    "# \treturn lrate\n",
    " \n",
    "\n",
    "#learning_rate=0.1\n",
    "#decay_rate=learning_rate/ 3\n",
    "\n",
    "#opt = keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=decay_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "       \n",
    "       # preprocessing_function=preprocess_input,\n",
    "        \n",
    "        adaptive_equalization=True, \n",
    "        #histogram_equalization=True,\n",
    "        rotation_range=90,\n",
    "        #brightness_range=[0.5,2],\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        #featurewise_center=True,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LR = 1e-4\n",
    "MAX_LR = 1e-3\n",
    "BATCH_SIZE = 16\n",
    "STEP_SIZE = 8\n",
    "CLR_METHOD = \"triangular\"\n",
    "NUM_EPOCHS = 5\n",
    "CLASSES=['abnormal','normal']\n",
    "\n",
    "\n",
    "# define the path to the output learning rate finder plot, training\n",
    "# history plot and cyclical learning rate plot\n",
    "LRFIND_PLOT_PATH = os.path.sep.join([\"output\", \"lrfind_plot.png\"])\n",
    "TRAINING_PLOT_PATH = os.path.sep.join([\"output\", \"training_loss.png\"])\n",
    "TRAINING_PLOT_ACC_PATH = os.path.sep.join([\"output\", \"training_acc.png\"])\n",
    "CLR_PLOT_PATH = os.path.sep.join([\"output\", \"clr_plot.png\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.1)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy',f1])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate=1\n",
    "if rate!=1:\n",
    "\tprint(\"[INFO] finding learning rate...\")\n",
    "\tlrf = LearningRateFinder(model)\n",
    "\tlrf.find(\n",
    "\t\ttrain_datagen.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "\t\t1e-12, 1e+1,epochs=5,\n",
    "\t\tstepsPerEpoch=np.ceil((len(trainX) / float(BATCH_SIZE))),\n",
    "\t\tbatchSize=BATCH_SIZE)\n",
    "\t \n",
    "\t\t# plot the loss for the various learning rates and save the\n",
    "\t\t# resulting plot to disk\n",
    "\tlrf.plot_loss()\n",
    "\tplt.savefig(LRFIND_PLOT_PATH)\n",
    "\tprint(\"[INFO] learning rate finder complete\")\n",
    "\tprint(\"[INFO] examine plot and adjust learning rates before training\")\n",
    "\t#sys.exit(0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "import math\n",
    "rates=[]\n",
    "stepSize = STEP_SIZE * (trainX.shape[0] // BATCH_SIZE)\n",
    "clr = CyclicLR(\n",
    "\tmode=CLR_METHOD,\n",
    "\tbase_lr=MIN_LR,\n",
    "\tmax_lr=MAX_LR,\n",
    "\tstep_size=stepSize)\n",
    "\n",
    "\n",
    "\n",
    "# initial rate is set based on learning rate finder max value\n",
    "initial_lrate=1e-1\n",
    "def time_decay(EPOCHS, initial_lrate):\n",
    "    decay_rate = 0.1\n",
    "    new_lrate = initial_lrate/(1+decay_rate*EPOCHS)\n",
    "    return new_lrate\n",
    "\n",
    "initial_lrate=1e-1\n",
    "def time_decay1(EPOCHS, initial_lrate):\n",
    "    decay_rate = 0.1\n",
    "    epochs_drop=3.0\n",
    "    new_lrate = initial_lrate/(1+decay_rate*math.floor((1+EPOCHS)/epochs_drop) )\n",
    "    rates.append(new_lrate)\n",
    "    return new_lrate\n",
    "\n",
    "def step_decay(EPOCHS):\n",
    "    initial_lrate=1e-1\n",
    "    drop=0.5\n",
    "    epochs_drop=3.0\n",
    "    new_lrate=initial_lrate*math.pow(drop,math.floor((1+EPOCHS)/epochs_drop))\n",
    "    return new_lrate\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "lrate = LearningRateScheduler(time_decay,verbose=1)\n",
    "\n",
    "lrate1 = LearningRateScheduler(step_decay,verbose=1)\n",
    "\n",
    "lrate2 = LearningRateScheduler(time_decay1,verbose=1)\n",
    "\n",
    "\n",
    "filepath=\" binary 33 Adam 0.01 binary weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "#early = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10)\n",
    "reduce1 = keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', verbose=1,factor= 0.33, patience=1,min_lr=1e-4)\n",
    "\n",
    "\n",
    "#lrate = LearningRateScheduler(step_decay, verbose=1)\n",
    "\n",
    "\n",
    "#callbacks_list = [checkpoint,early,reduce1]\n",
    "\n",
    "#callbacks_list = [checkpoint,reduce1,lrate]\n",
    "\n",
    "#callbacks_list = [checkpoint,reduce1]\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint,lrate2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#validation_generator = val_datagen.flow(testX, testY)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from sklearn.utils.multiclass import type_of_target\n",
    "# type_of_target(labels1)\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# labels1 = label_encoder.fit_transform(labels)\n",
    "\n",
    "#print(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network... FOLD No: 0\n",
      "Epoch 1/5\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 158s - loss: 72515.9462 - accuracy: 0.4798 - f1: 0.4787 - val_loss: 22.4157 - val_accuracy: 0.8913 - val_f1: 0.8854\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.89130, saving model to  binary 33 Adam 0.01 binary weights-improvement-01-0.89.hdf5\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 151s - loss: 45.4200 - accuracy: 0.3082 - f1: 0.3079 - val_loss: 0.9094 - val_accuracy: 0.2337 - val_f1: 0.2448\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.89130\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.09090909226374191.\n",
      " - 154s - loss: 56.6618 - accuracy: 0.2720 - f1: 0.2721 - val_loss: 1.1651 - val_accuracy: 0.2337 - val_f1: 0.2448\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.89130\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0826446305621754.\n",
      " - 157s - loss: 531.6809 - accuracy: 0.2734 - f1: 0.2725 - val_loss: 1.1407 - val_accuracy: 0.2337 - val_f1: 0.2448\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.89130\n",
      "Epoch 5/5\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.07513148540800267.\n",
      " - 160s - loss: 1.7704 - accuracy: 0.2636 - f1: 0.2631 - val_loss: 1.2437 - val_accuracy: 0.2337 - val_f1: 0.2448\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89130\n",
      "[INFO] training network... FOLD No: 1\n",
      "Epoch 1/5\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 158s - loss: 1.8275 - accuracy: 0.2819 - f1: 0.2819 - val_loss: 1.4459 - val_accuracy: 0.2011 - val_f1: 0.1979\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89130\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 149s - loss: 3.5084 - accuracy: 0.2759 - f1: 0.2752 - val_loss: 1.1160 - val_accuracy: 0.2011 - val_f1: 0.1979\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.89130\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.09090909226374191.\n",
      " - 151s - loss: 1.7972 - accuracy: 0.2845 - f1: 0.2843 - val_loss: 1.3326 - val_accuracy: 0.2011 - val_f1: 0.1979\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.89130\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0826446305621754.\n",
      " - 155s - loss: 1.7823 - accuracy: 0.2650 - f1: 0.2645 - val_loss: 1.2541 - val_accuracy: 0.2011 - val_f1: 0.1979\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.89130\n",
      "Epoch 5/5\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.07513148540800267.\n",
      " - 157s - loss: 1.8045 - accuracy: 0.2833 - f1: 0.2833 - val_loss: 1.3090 - val_accuracy: 0.2011 - val_f1: 0.1979\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89130\n",
      "[INFO] training network... FOLD No: 2\n",
      "Epoch 1/5\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 155s - loss: 1.7875 - accuracy: 0.2636 - f1: 0.2641 - val_loss: 1.2473 - val_accuracy: 0.2717 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89130\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 155s - loss: 1.7840 - accuracy: 0.2538 - f1: 0.2537 - val_loss: 1.1913 - val_accuracy: 0.2717 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.89130\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.09090909226374191.\n",
      " - 148s - loss: 1.9150 - accuracy: 0.2650 - f1: 0.2655 - val_loss: 1.2707 - val_accuracy: 0.2717 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.89130\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0826446305621754.\n",
      " - 148s - loss: 2.1082 - accuracy: 0.2664 - f1: 0.2659 - val_loss: 1.2824 - val_accuracy: 0.2717 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.89130\n",
      "Epoch 5/5\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.07513148540800267.\n",
      " - 148s - loss: 1.7771 - accuracy: 0.2636 - f1: 0.2641 - val_loss: 1.1697 - val_accuracy: 0.2717 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89130\n",
      "[INFO] training network... FOLD No: 3\n",
      "Epoch 1/5\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 154s - loss: 1.7960 - accuracy: 0.2636 - f1: 0.2635 - val_loss: 1.2727 - val_accuracy: 0.2609 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89130\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 149s - loss: 1.7850 - accuracy: 0.2664 - f1: 0.2659 - val_loss: 1.1833 - val_accuracy: 0.2609 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.89130\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.09090909226374191.\n",
      " - 150s - loss: 1.7988 - accuracy: 0.2678 - f1: 0.2686 - val_loss: 1.2430 - val_accuracy: 0.2609 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.89130\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0826446305621754.\n",
      " - 151s - loss: 1.7774 - accuracy: 0.2608 - f1: 0.2607 - val_loss: 1.3021 - val_accuracy: 0.2609 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.89130\n",
      "Epoch 5/5\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.07513148540800267.\n",
      " - 158s - loss: 1.7817 - accuracy: 0.2664 - f1: 0.2656 - val_loss: 1.2366 - val_accuracy: 0.2609 - val_f1: 0.2760\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89130\n",
      "[INFO] training network... FOLD No: 4\n",
      "Epoch 1/5\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 167s - loss: 1.7837 - accuracy: 0.2664 - f1: 0.2659 - val_loss: 0.9328 - val_accuracy: 0.2500 - val_f1: 0.2552\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.89130\n",
      "Epoch 2/5\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.10000000149011612.\n",
      " - 153s - loss: 1.8050 - accuracy: 0.2692 - f1: 0.2690 - val_loss: 1.3197 - val_accuracy: 0.2500 - val_f1: 0.2552\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.89130\n",
      "Epoch 3/5\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.09090909226374191.\n",
      " - 157s - loss: 1.7868 - accuracy: 0.2762 - f1: 0.2753 - val_loss: 1.2000 - val_accuracy: 0.2500 - val_f1: 0.2552\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.89130\n",
      "Epoch 4/5\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0826446305621754.\n",
      " - 154s - loss: 1.7808 - accuracy: 0.2594 - f1: 0.2590 - val_loss: 1.2865 - val_accuracy: 0.2500 - val_f1: 0.2552\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.89130\n",
      "Epoch 5/5\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.07513148540800267.\n",
      " - 148s - loss: 1.7966 - accuracy: 0.2692 - f1: 0.2697 - val_loss: 1.4835 - val_accuracy: 0.2500 - val_f1: 0.2552\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.89130\n"
     ]
    }
   ],
   "source": [
    "#kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "\n",
    "class_weights = {0: 1.,\n",
    "                1: 10.\n",
    "                }\n",
    "\n",
    "EPOCHS = 5\n",
    "seed = 7\n",
    "foldno=5\n",
    "for i in range(foldno):\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.1)\n",
    "    model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy',f1])\n",
    "    (trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "        test_size=0.20, random_state=np.random.randint(1,200, 1)[0], shuffle=True)\n",
    "    print(\"[INFO] training network... FOLD No: \" +str(i))\n",
    "    H = model.fit_generator(\n",
    "        train_datagen.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "        validation_data=(testX, testY),\n",
    "        steps_per_epoch=trainX.shape[0] // BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks_list,\n",
    "        class_weight=class_weights,\n",
    "        verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "#INIT_LR = 1e-1\n",
    "#BS = 64\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network and show a classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
    "print(classification_report(testY.argmax(axis=\n",
    "                                         1),\n",
    "\tpredictions.argmax(axis=1), target_names=CLASSES))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "#plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "#plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss \")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(TRAINING_PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "#plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "#plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(TRAINING_PLOT_ACC_PATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "#plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "#plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, rates, label=\"learning rates\")\n",
    "#plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Learning rate scheduler\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the learning rate history\n",
    "# N = np.arange(0, len(clr.history[\"lr\"]))\n",
    "# plt.figure()\n",
    "# plt.plot(N, clr.history[\"lr\"])\n",
    "# plt.title(\"Cyclical Learning Rate (CLR)\")\n",
    "# plt.xlabel(\"Training Iterations\")\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.savefig(CLR_PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
